# Copyright 2022 The Johns Hopkins University Applied Physics Laboratory
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# lambdafcns contains symbolic links to lambda functions in boss-tools/lambda.
# Since lambda is a reserved word, this allows importing from that folder
# without updating scripts responsible for deploying the lambda code.

from lambdafcns import ingest_queue_upload_volumetric_lambda as iqu

import hashlib
from hypothesis import example, given
from hypothesis import strategies as st
import json
import math
from typing import Dict, Tuple
import unittest
from unittest.mock import patch, MagicMock

# We'll only test this tile size for now.  For volumetric, the tile size should
# be a multiple of the cuboid size.
TILE_SIZE = 1024

ONLY_X_SKIPS = {
    "x_start": 0,
    "x_stop": 4096,
    "y_start": 0,
    "y_stop": 4096,
    "z_start": 0,
    "z_stop": 128,
    "items_to_skip": 2,
}

ONLY_Y_SKIPS = {
    "x_start": 0,
    "x_stop": 4096,
    "y_start": 0,
    "y_stop": 4096,
    "z_start": 0,
    "z_stop": 128,
    "items_to_skip": 8,
}

@st.composite
def start_stop_values(draw, cuboid_size: int, tile_size: int) -> Tuple[int, int]:
    """
    Hypothesis strategy for generating start-stop ranges for the ingest
    coordinate space.

    Args:
        draw: Hypothesis function for that gets a single value.
        cuboid_size: Size of the cuboid along one dimension.
        tile_size: Number of cuboids that form a "tile" for the ingest.

    Returns:
        Tuple[start, stop]
    """
    start = draw(st.integers(0, 2048)) * cuboid_size
    # Assume we'll always have a stop of at least the TILE_SIZE.
    stop = draw(st.integers(start + tile_size, max(tile_size, start + 20 * cuboid_size)))
    return start, stop


@st.composite
def xyz_start_stops_and_num_skips(draw) -> Dict[str, int]:
    """
    Hypothesis strategy for generating start-stop ranges and number of items to
    skip for the volumetric ingests.

    Args:
        draw: Hypothesis function for that gets a single value.

    Returns:
        Dict
    """
    x = draw(start_stop_values(iqu.CUBOID_X, TILE_SIZE))
    y = draw(start_stop_values(iqu.CUBOID_Y, TILE_SIZE))
    z = draw(start_stop_values(iqu.CUBOID_Z, 1))

    num_x_tiles = (x[1] - x[0]) / TILE_SIZE
    num_y_tiles = (y[1] - y[0]) / TILE_SIZE
    num_xy_tiles = num_x_tiles * num_y_tiles
    num_z_planes = z[1] - z[0]
    num_skips = draw(st.integers(0, int(num_xy_tiles * num_z_planes)))

    return {
        "x_start": x[0],
        "x_stop": x[1],
        "y_start": y[0],
        "y_stop": y[1],
        "z_start": z[0],
        "z_stop": z[1],
        "items_to_skip": num_skips,
    }


class TestIngestQueueUploadLambda(unittest.TestCase):

    def tile_count(self, kwargs):
        x = math.ceil((kwargs["x_stop"] - kwargs["x_start"]) / kwargs["x_tile_size"])
        y = math.ceil((kwargs["y_stop"] - kwargs["y_start"]) / kwargs["y_tile_size"])
        z = math.ceil((kwargs["z_stop"] - kwargs["z_start"]) / kwargs["z_chunk_size"])
        t = math.ceil((kwargs["t_stop"] - kwargs["t_start"]))
        return x * y * z * t

    def test_all_messages_are_there(self):
        """
        This test will show that when items are being genereated by multiple lambdas, the sum of those items
        will be the exact same set that would be generated by a single lambda creating them all.
        Test_all_messages_are_there tests() first it creates
        set of messages that all fit into a single lambda populating a dictionary with all the values returned.
        Then it runs the create_messages 4 more times each time with the appropriate items_to_skip and
        MAX_NUM_ITEMS_PER_LAMBDA set. It pulls the tile key out of the dictionary to verify that all the items were
        accounted for.  In the end there should be no items left in the dictionary.

        This test can with many different values for tile sizes and starts and stop vaules and num_lambdas can be
        changed.
        Args:
            fake_resource:

        Returns:

        """

        args = {
            "upload_sfn": "IngestUpload",
            "x_start": 0,
            "x_stop": 2048,
            "y_start": 0,
            "y_stop": 2048,
            "z_start": 0,
            "z_stop": 128,
            "t_start": 0,
            "t_stop": 1,
            "project_info": [
              "1",
              "2",
              "3"
            ],
            "ingest_queue": "https://queue.amazonaws.com/...",
            "job_id": 11,
            "upload_queue": "https://queue.amazonaws.com/...",
            "x_tile_size": TILE_SIZE,
            "y_tile_size": TILE_SIZE,
            "t_tile_size": 1,
            "z_tile_size": 1,
            "resolution": 0,
            "items_to_skip": 0,
            'MAX_NUM_ITEMS_PER_LAMBDA': 500000,
            'z_chunk_size': 64
        }

        # Walk create_messages generator as a single lambda would and populate the dictionary with all Keys like this
        # "Chunk --- items".
        dict = {}
        msgs = iqu.create_messages(args)
        for msg_json in msgs:
            ct_key = self.generate_chunk_tile_key(msg_json)
            print(ct_key)
            if ct_key not in dict:
                dict[ct_key] = 1
            else:
                self.fail(f"Dictionary already contains key: {ct_key}")

        # Verify correct count of items in dictionary
        dict_length = len(dict.keys())
        item_count = self.tile_count(args)
        print("Item Count: {}".format(item_count))
        self.assertEqual(dict_length, item_count)

        # loop through create_messages() num_lambda times pulling out each tile from the dictionary.
        num_lambdas = 2
        args["MAX_NUM_ITEMS_PER_LAMBDA"] = math.ceil(dict_length / num_lambdas)
        for skip in range(0, dict_length, args["MAX_NUM_ITEMS_PER_LAMBDA"]):
            args["items_to_skip"] = skip
            # print("Skip: " + str(skip))
            msgs = iqu.create_messages(args)
            for msg_json in msgs:
                ct_key = self.generate_chunk_tile_key(msg_json)
                if ct_key in dict:
                    del dict[ct_key]
                else:
                    self.fail(f"Dictionary does not contains key: {ct_key}")

        # Verify Dictionary has no left over items.
        self.assertEqual(len(dict), 0)

    def test_new_msg_creation_no_skips(self):
        args = {
            "upload_sfn": "IngestUpload",
            "x_start": 0,
            "x_stop": 2048,
            "y_start": 0,
            "y_stop": 2048,
            "z_start": 0,
            "z_stop": 128,
            "t_start": 0,
            "t_stop": 1,
            "project_info": [
              "1",
              "2",
              "3"
            ],
            "ingest_queue": "https://queue.amazonaws.com/...",
            "job_id": 11,
            "upload_queue": "https://queue.amazonaws.com/...",
            "x_tile_size": TILE_SIZE,
            "y_tile_size": TILE_SIZE,
            "t_tile_size": 1,
            "z_tile_size": 1,
            "resolution": 0,
            "items_to_skip": 0,
            'MAX_NUM_ITEMS_PER_LAMBDA': 500000,
            'z_chunk_size': 64
        }

        orig_msgs = orig_create_messages(args)
        orig_keys = set()
        for m in orig_msgs:
            key = self.generate_chunk_tile_key(m)
            self.assertNotIn(key, orig_keys)
            orig_keys.add(key)

        new_msgs = iqu.create_messages(args)
        new_keys = set()
        for m in new_msgs:
            key = self.generate_chunk_tile_key(m)
            self.assertNotIn(key, new_keys)
            new_keys.add(key)

        self.assertSetEqual(orig_keys, new_keys)

    @given(params=xyz_start_stops_and_num_skips())
    @example(params=ONLY_X_SKIPS)
    @example(params=ONLY_Y_SKIPS)
    def test_new_msg_creation_fuzz(self, params):
        args = {
            "upload_sfn": "IngestUpload",
            "x_start": params["x_start"],
            "x_stop": params["x_stop"],
            "y_start": params["y_start"],
            "y_stop": params["y_stop"],
            "z_start": params["z_start"],
            "z_stop": params["z_stop"],
            "t_start": 0,
            "t_stop": 1,
            "project_info": [
              "1",
              "2",
              "3"
            ],
            "ingest_queue": "https://queue.amazonaws.com/...",
            "job_id": 11,
            "upload_queue": "https://queue.amazonaws.com/...",
            "x_tile_size": TILE_SIZE,
            "y_tile_size": TILE_SIZE,
            "t_tile_size": 1,
            "z_tile_size": 1,
            "resolution": 0,
            "items_to_skip": params["items_to_skip"],
            'MAX_NUM_ITEMS_PER_LAMBDA': 500000,
            'z_chunk_size': 64
        }
        print(f"{params['x_start']}:{params['x_stop']}, {params['y_start']}:{params['y_stop']}, {params['z_start']}:{params['z_stop']}, skip: {params['items_to_skip']}")

        orig_msgs = orig_create_messages(args)
        orig_keys = set()
        for m in orig_msgs:
            key = self.generate_chunk_tile_key(m)
            self.assertNotIn(key, orig_keys)
            orig_keys.add(key)

        new_msgs = iqu.create_messages(args)
        new_keys = set()
        for m in new_msgs:
            key = self.generate_chunk_tile_key(m)
            self.assertNotIn(key, new_keys)
            new_keys.add(key)

        self.assertSetEqual(orig_keys, new_keys)

    def generate_chunk_tile_key(self, msg_json):
        """
        Generate a key to track messages for testing.

        Args:
            msg_json (str): JSON message encoded as string intended for the upload queue.

        Returns:
            (str): Unique key identifying message.
        """
        msg = json.loads(msg_json)
        return msg["chunk_key"]


def orig_create_messages(args):
    """Create all of the tile messages to be enqueued.  Currently not support t extent.

    This is the original message creation function.  It was replaced because it
    is very slow when there are many items to skip.

    Args:
        args (dict): Same arguments as handler()

    Returns:
        list: List of strings containing Json data
    """

    tile_size = lambda v: args[v + "_tile_size"]
    # range_ does not work with z. Need to use z_chunk_size instead with volumetric ingest
    range_ = lambda v: range(args[v + '_start'], args[v + '_stop'], tile_size(v))

    # DP NOTE: generic version of
    # BossBackend.encode_chunk_key and BiossBackend.encode.tile_key
    # from ingest-client/ingestclient/core/backend.py
    def hashed_key(*args):
        base = '&'.join(map(str,args))

        md5 = hashlib.md5()
        md5.update(base.encode())
        digest = md5.hexdigest()

        return '&'.join([digest, base])

    chunks_to_skip = args['items_to_skip']
    count_in_offset = 0
    for t in range_('t'):
        for z in range(args['z_start'], args['z_stop'], args['z_chunk_size']):
            for y in range_('y'):
                for x in range_('x'):

                    if chunks_to_skip > 0:
                        chunks_to_skip -= 1
                        continue

                    chunk_x = int(x / tile_size('x'))
                    chunk_y = int(y / tile_size('y'))
                    chunk_z = int(z / args['z_chunk_size'])
                    chunk_key = hashed_key(1,  # num of items
                                           args['project_info'][0],
                                           args['project_info'][1],
                                           args['project_info'][2],
                                           args['resolution'],
                                           chunk_x,
                                           chunk_y,
                                           chunk_z,
                                           t)

                    count_in_offset += 1
                    if count_in_offset > args['MAX_NUM_ITEMS_PER_LAMBDA']:
                        return  # end the generator

                    cuboids = []

                    # Currently, only allow ingest for time sample 0.
                    t = 0
                    lookup_key = iqu.lookup_key_from_chunk_key(chunk_key)
                    res = iqu.resolution_from_chunk_key(chunk_key)

                    for chunk_offset_z in range(0, args["z_chunk_size"], iqu.CUBOID_Z):
                        for chunk_offset_y in range(0, tile_size('y'), iqu.CUBOID_Y):
                            for chunk_offset_x in range(0, tile_size('x'), iqu.CUBOID_X):
                                morton = iqu.XYZMorton(
                                    [(x+chunk_offset_x)/iqu.CUBOID_X, (y+chunk_offset_y)/iqu.CUBOID_Y, (z+chunk_offset_z)/iqu.CUBOID_Z])
                                object_key = iqu.generate_object_key(lookup_key, res, t, morton)
                                new_cuboid = {
                                    "x": chunk_offset_x,
                                    "y": chunk_offset_y,
                                    "z": chunk_offset_z,
                                    "key": object_key
                                }
                                cuboids.append(new_cuboid)

                    msg = {
                        'chunk_key': chunk_key,
                        'cuboids': cuboids,
                    }

                    yield json.dumps(msg)
